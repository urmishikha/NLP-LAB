{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b7bdc1-09ea-425e-ae06-d803672fbbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in ./anaconda3/lib/python3.11/site-packages (37.1.0)\n",
      "Requirement already satisfied: tzdata in ./anaconda3/lib/python3.11/site-packages (from faker) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38ae366-9b90-4654-b195-839b1185d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset with 1500 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for realistic data generation\n",
    "fake = Faker()\n",
    "\n",
    "# Define our intents and sample queries\n",
    "INTENT_TEMPLATES = {\n",
    "    'order_status': [\n",
    "        (\"What's the status of order #{order_id}?\", {'order_id': '{order_id}'}),\n",
    "        (\"Can you check where order {order_id} is?\", {'order_id': '{order_id}'}),\n",
    "        (\"Has order #{order_id} shipped yet?\", {'order_id': '{order_id}'})\n",
    "    ],\n",
    "    'product_inquiry': [\n",
    "        (\"When will {product} be back in stock?\", {'product': '{product}'}),\n",
    "        (\"Do you have {product} in {color}?\", {'product': '{product}', 'color': '{color}'}),\n",
    "        (\"What are the specs for the {product}?\", {'product': '{product}'})\n",
    "    ],\n",
    "    'refund_request': [\n",
    "        (\"I want to return my {product}\", {'product': '{product}'}),\n",
    "        (\"The {product} arrived damaged, I need a refund\", {'product': '{product}', 'issue': 'damaged'}),\n",
    "        (\"How do I return {product}?\", {'product': '{product}'})\n",
    "    ],\n",
    "    'account_help': [\n",
    "        (\"I can't login to my account\", {}),\n",
    "        (\"My password isn't working\", {}),\n",
    "        (\"How do I reset my password?\", {})\n",
    "    ],\n",
    "    'technical_support': [\n",
    "        (\"The app crashes when I {action}\", {'action': '{action}'}),\n",
    "        (\"I'm getting error {error_code} when trying to {action}\", \n",
    "         {'error_code': '{error_code}', 'action': '{action}'}),\n",
    "        (\"The website won't let me {action}\", {'action': '{action}'})\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define possible values for placeholders\n",
    "PRODUCTS = ['iPhone', 'Samsung Galaxy', 'MacBook Pro', 'AirPods', 'PlayStation 5', \n",
    "            'Nike Air Force', 'Dyson vacuum', 'Instant Pot', 'Kindle', 'Fitbit']\n",
    "COLORS = ['black', 'white', 'blue', 'red', 'silver', 'space gray']\n",
    "ACTIONS = ['checkout', 'login', 'add to cart', 'view my orders', 'update payment info']\n",
    "ERROR_CODES = ['404', '500', 'ERR_CONNECTION_REFUSED', 'ERR_TIMEOUT']\n",
    "\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Select a random intent\n",
    "        intent = random.choice(list(INTENT_TEMPLATES.keys()))\n",
    "        # Select a random template for this intent\n",
    "        template, entities = random.choice(INTENT_TEMPLATES[intent])\n",
    "        \n",
    "        # Replace placeholders with realistic values\n",
    "        query = template\n",
    "        entity_values = {}\n",
    "        \n",
    "        if '{order_id}' in template:\n",
    "            order_id = fake.bothify(text='??#####').upper()\n",
    "            query = query.replace('{order_id}', order_id)\n",
    "            if entities.get('order_id') == '{order_id}':\n",
    "                entity_values['order_id'] = order_id\n",
    "        \n",
    "        if '{product}' in template:\n",
    "            product = random.choice(PRODUCTS)\n",
    "            query = query.replace('{product}', product)\n",
    "            if entities.get('product') == '{product}':\n",
    "                entity_values['product'] = product\n",
    "                \n",
    "        if '{color}' in template:\n",
    "            color = random.choice(COLORS)\n",
    "            query = query.replace('{color}', color)\n",
    "            if entities.get('color') == '{color}':\n",
    "                entity_values['color'] = color\n",
    "                \n",
    "        if '{action}' in template:\n",
    "            action = random.choice(ACTIONS)\n",
    "            query = query.replace('{action}', action)\n",
    "            if entities.get('action') == '{action}':\n",
    "                entity_values['action'] = action\n",
    "                \n",
    "        if '{error_code}' in template:\n",
    "            error_code = random.choice(ERROR_CODES)\n",
    "            query = query.replace('{error_code}', error_code)\n",
    "            if entities.get('error_code') == '{error_code}':\n",
    "                entity_values['error_code'] = error_code\n",
    "        \n",
    "        # Add some natural language variations\n",
    "        query = add_natural_variations(query)\n",
    "        \n",
    "        samples.append({\n",
    "            'text': query,\n",
    "            'intent': intent,\n",
    "            'entities': entity_values\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "def add_natural_variations(text):\n",
    "    \"\"\"Add natural language variations to make queries more realistic\"\"\"\n",
    "    variations = [\n",
    "        (\"Can you tell me \", \"\"),\n",
    "        (\"I was wondering \", \"\"),\n",
    "        (\"\", \" please\"),\n",
    "        (\"\", \" thanks\"),\n",
    "        (\"\", \" thank you\"),\n",
    "        (\"Hi, \", \"\"),\n",
    "        (\"Hello, \", \"\"),\n",
    "        (\"Hey, \", \"\"),\n",
    "        (\"\", \"?\"),\n",
    "        (\"\", \"...\"),\n",
    "    ]\n",
    "    prefix, suffix = random.choice(variations)\n",
    "    return prefix + text + suffix\n",
    "\n",
    "# Generate our dataset\n",
    "df = generate_synthetic_data(1500)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('synthetic_customer_support.csv', index=False)\n",
    "print(f\"Generated dataset with {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becfe471-e47b-4692-8572-fc7bc1ad0507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intent Distribution:\n",
      "intent\n",
      "order_status         307\n",
      "technical_support    306\n",
      "account_help         305\n",
      "product_inquiry      299\n",
      "refund_request       283\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Queries:\n",
      "\n",
      "Query: What's the status of order #GC19605? please\n",
      "Intent: order_status\n",
      "Entities: {'order_id': 'GC19605'}\n",
      "\n",
      "Query: Hey, How do I return Dyson vacuum?\n",
      "Intent: refund_request\n",
      "Entities: {'product': 'Dyson vacuum'}\n",
      "\n",
      "Query: Do you have Nike Air Force in silver? thanks\n",
      "Intent: product_inquiry\n",
      "Entities: {'product': 'Nike Air Force', 'color': 'silver'}\n",
      "\n",
      "Query: How do I return Nike Air Force? please\n",
      "Intent: refund_request\n",
      "Entities: {'product': 'Nike Air Force'}\n",
      "\n",
      "Query: Hi, How do I reset my password?\n",
      "Intent: account_help\n",
      "Entities: {}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('synthetic_customer_support.csv')\n",
    "\n",
    "# Convert string representation of entities to dictionary\n",
    "import ast\n",
    "df['entities'] = df['entities'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Show distribution of intents\n",
    "print(\"\\nIntent Distribution:\")\n",
    "print(df['intent'].value_counts())\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample Queries:\")\n",
    "for _, row in df.sample(5).iterrows():\n",
    "    print(f\"\\nQuery: {row['text']}\")\n",
    "    print(f\"Intent: {row['intent']}\")\n",
    "    print(f\"Entities: {row['entities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390f980d-92e3-47ce-b1af-1a1198ef1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intent Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     account_help       1.00      1.00      1.00        54\n",
      "     order_status       1.00      1.00      1.00        67\n",
      "  product_inquiry       1.00      1.00      1.00        56\n",
      "   refund_request       1.00      1.00      1.00        65\n",
      "technical_support       1.00      1.00      1.00        58\n",
      "\n",
      "         accuracy                           1.00       300\n",
      "        macro avg       1.00      1.00      1.00       300\n",
      "     weighted avg       1.00      1.00      1.00       300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['intent'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000, multi_class='ovr')\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(\"\\nIntent Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(clf, 'intent_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "380daf08-d6cd-40e6-923d-da793c0bb1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/urmisikhadash/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/urmisikhadash/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.3758\n",
      "Epoch 2 - Average Loss: 0.0089\n",
      "Epoch 3 - Average Loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('intent_classifier_distilbert/tokenizer_config.json',\n",
       " 'intent_classifier_distilbert/special_tokens_map.json',\n",
       " 'intent_classifier_distilbert/vocab.txt',\n",
       " 'intent_classifier_distilbert/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load and prepare your dataset\n",
    "df = pd.read_csv('synthetic_customer_support.csv')\n",
    "texts = df['text'].values\n",
    "intents = df['intent'].values\n",
    "\n",
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(intents)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# 2. Create a PyTorch Dataset class\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_classes\n",
    ")\n",
    "\n",
    "# 4. Create train dataset and dataloader\n",
    "train_dataset = IntentDataset(texts, encoded_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 5. Training setup\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 6. Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('intent_classifier_distilbert')\n",
    "tokenizer.save_pretrained('intent_classifier_distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4003ec48-e1c5-4ca3-884a-00f40185655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (77.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b67b9b6-05c8-470a-bb02-9e201027dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity Extraction Example:\n",
      "Text: I'm getting error 404 when trying to checkout my order #AB1234 for the black iPhone\n",
      "Entities: {'ERROR_CODE': 'AB1234', 'COLOR': 'black', 'PRODUCT': 'iPhone'}\n",
      "\n",
      "DataFrame with extracted entities:\n",
      "                               text                       predicted_entities\n",
      "0           Status of order #AB1234                 {'ERROR_CODE': 'AB1234'}\n",
      "1  I want to return my black iPhone  {'COLOR': 'black', 'PRODUCT': 'iPhone'}\n",
      "2       Error 500 when checking out                    {'ERROR_CODE': '500'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "\n",
    "# First install the model if needed\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"Downloading language model...\")\n",
    "    from spacy.cli import download\n",
    "    download('en_core_web_sm')\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define your product and color lists (example values)\n",
    "PRODUCTS = ['iPhone', 'Samsung', 'MacBook', 'AirPods', 'PlayStation']\n",
    "COLORS = ['black', 'white', 'blue', 'red', 'silver']\n",
    "\n",
    "# Initialize matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define patterns for our entities\n",
    "patterns = {\n",
    "    'ORDER_ID': [\n",
    "        [{'TEXT': {'REGEX': r'[A-Z]{2}\\d{5}'}}],\n",
    "        [{'TEXT': {'REGEX': r'#?\\d{5,8}'}}]\n",
    "    ],\n",
    "    'PRODUCT': [\n",
    "        [{'LOWER': {'IN': [p.lower() for p in PRODUCTS]}}]\n",
    "    ],\n",
    "    'COLOR': [\n",
    "        [{'LOWER': {'IN': COLORS}}]\n",
    "    ],\n",
    "    'ERROR_CODE': [\n",
    "        [{'TEXT': {'REGEX': r'\\d{3}'}}],\n",
    "        [{'TEXT': {'REGEX': r'ERR_[A-Z_]+'}}]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add patterns to matcher\n",
    "for label, pattern in patterns.items():\n",
    "    matcher.add(label, pattern)\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    entities = {}\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        entities[label] = span.text\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test entity extraction\n",
    "sample_text = \"I'm getting error 404 when trying to checkout my order #AB1234 for the black iPhone\"\n",
    "print(\"\\nEntity Extraction Example:\")\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Entities: {extract_entities(sample_text)}\")\n",
    "\n",
    "# Example dataframe (replace with your actual dataframe)\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Status of order #AB1234\",\n",
    "        \"I want to return my black iPhone\",\n",
    "        \"Error 500 when checking out\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Add entity extraction to dataframe\n",
    "df['predicted_entities'] = df['text'].apply(extract_entities)\n",
    "print(\"\\nDataFrame with extracted entities:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d3100b5-9c6f-4721-9894-46ca1b7afcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot Test:\n",
      "\n",
      "Query: What's the status of order #AB1234?\n",
      "Intent: order_status\n",
      "Entities: {'ERROR_CODE': 'AB1234'}\n",
      "Response: Order  was delivered yesterday.\n",
      "\n",
      "Query: Do you have the iPhone in black?\n",
      "Intent: product_inquiry\n",
      "Entities: {'PRODUCT': 'iPhone', 'COLOR': 'black'}\n",
      "Response: The iPhone is currently in stock.\n",
      "\n",
      "Query: I need to return my defective MacBook Pro\n",
      "Intent: refund_request\n",
      "Entities: {'PRODUCT': 'MacBook'}\n",
      "Response: I can help you return the MacBook.\n",
      "\n",
      "Query: I'm getting error 500 when trying to login\n",
      "Intent: technical_support\n",
      "Entities: {'ERROR_CODE': '500'}\n",
      "Response: I've noted the 500 you're experiencing.\n"
     ]
    }
   ],
   "source": [
    "class CustomerSupportChatbot:\n",
    "    def __init__(self):\n",
    "        # Load intent classifier\n",
    "        self.vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "        self.intent_clf = joblib.load('intent_classifier.joblib')\n",
    "        \n",
    "        # Load entity recognizer (spaCy)\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        for label, pattern in patterns.items():\n",
    "            self.matcher.add(label, pattern)\n",
    "    \n",
    "    def predict_intent(self, text):\n",
    "        # Vectorize text\n",
    "        X = self.vectorizer.transform([text])\n",
    "        # Predict intent\n",
    "        return self.intent_clf.predict(X)[0]\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        matches = self.matcher(doc)\n",
    "        entities = {}\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            label = self.nlp.vocab.strings[match_id]\n",
    "            span = doc[start:end]\n",
    "            entities[label] = span.text\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def generate_response(self, intent, entities):\n",
    "        # Simple response generation\n",
    "        responses = {\n",
    "            'order_status': [\n",
    "                f\"I've located your order {entities.get('ORDER_ID', '')}. It's currently being processed.\",\n",
    "                f\"Your order {entities.get('ORDER_ID', '')} is out for delivery.\",\n",
    "                f\"Order {entities.get('ORDER_ID', '')} was delivered yesterday.\"\n",
    "            ],\n",
    "            'product_inquiry': [\n",
    "                f\"The {entities.get('PRODUCT', 'product')} is currently in stock.\",\n",
    "                f\"We expect more {entities.get('PRODUCT', 'product')} inventory next week.\",\n",
    "                f\"The {entities.get('PRODUCT', 'product')} comes in {entities.get('COLOR', 'multiple')} colors.\"\n",
    "            ],\n",
    "            'refund_request': [\n",
    "                f\"I can help you return the {entities.get('PRODUCT', 'item')}.\",\n",
    "                f\"Please package the {entities.get('PRODUCT', 'item')} for return shipping.\",\n",
    "                f\"We'll process a refund for your {entities.get('PRODUCT', 'item')}.\"\n",
    "            ],\n",
    "            'account_help': [\n",
    "                \"I can help reset your password.\",\n",
    "                \"Please check your email for a password reset link.\",\n",
    "                \"Let me transfer you to account support.\"\n",
    "            ],\n",
    "            'technical_support': [\n",
    "                f\"I've noted the {entities.get('ERROR_CODE', 'error')} you're experiencing.\",\n",
    "                \"Our technical team is working on this issue.\",\n",
    "                \"Please try clearing your cache and cookies.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return random.choice(responses.get(intent, [\"I'm sorry, I didn't understand that.\"]))\n",
    "    \n",
    "    def process_query(self, text):\n",
    "        intent = self.predict_intent(text)\n",
    "        entities = self.extract_entities(text)\n",
    "        response = self.generate_response(intent, entities)\n",
    "        \n",
    "        return {\n",
    "            'intent': intent,\n",
    "            'entities': entities,\n",
    "            'response': response\n",
    "        }\n",
    "\n",
    "# Initialize and test the chatbot\n",
    "chatbot = CustomerSupportChatbot()\n",
    "\n",
    "test_queries = [\n",
    "    \"What's the status of order #AB1234?\",\n",
    "    \"Do you have the iPhone in black?\",\n",
    "    \"I need to return my defective MacBook Pro\",\n",
    "    \"I'm getting error 500 when trying to login\"\n",
    "]\n",
    "\n",
    "print(\"\\nChatbot Test:\")\n",
    "for query in test_queries:\n",
    "    result = chatbot.process_query(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Intent: {result['intent']}\")\n",
    "    print(f\"Entities: {result['entities']}\")\n",
    "    print(f\"Response: {result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48d0a92c-8981-41b0-a1a5-df1eaf615331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluated on 50 samples\n",
      "System Evaluation:\n",
      "Intent Accuracy: 1.00\n",
      "Appropriate Response Rate: 1.00\n",
      "\n",
      "Intent Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     account_help       1.00      1.00      1.00        10\n",
      "     order_status       1.00      1.00      1.00        11\n",
      "  product_inquiry       1.00      1.00      1.00         8\n",
      "   refund_request       1.00      1.00      1.00         7\n",
      "technical_support       1.00      1.00      1.00        14\n",
      "\n",
      "         accuracy                           1.00        50\n",
      "        macro avg       1.00      1.00      1.00        50\n",
      "     weighted avg       1.00      1.00      1.00        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('synthetic_customer_support.csv')\n",
    "\n",
    "# Check dataset size and adjust sample size accordingly\n",
    "sample_size = min(50, len(df))  # Take 50 or whatever is available if less than 50\n",
    "test_samples = df.sample(sample_size, random_state=42)  # Fixed random state for reproducibility\n",
    "\n",
    "true_intents = []\n",
    "pred_intents = []\n",
    "correct_responses = 0\n",
    "\n",
    "for _, row in test_samples.iterrows():\n",
    "    try:\n",
    "        result = chatbot.process_query(row['text'])\n",
    "        true_intents.append(row['intent'])\n",
    "        pred_intents.append(result['intent'])\n",
    "        \n",
    "        # Simple check if response seems appropriate\n",
    "        if result['intent'] == row['intent']:\n",
    "            correct_responses += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {row['text']}\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Only calculate metrics if we have samples\n",
    "if len(true_intents) > 0:\n",
    "    # Calculate accuracy\n",
    "    intent_accuracy = accuracy_score(true_intents, pred_intents)\n",
    "    response_accuracy = correct_responses / len(test_samples)\n",
    "\n",
    "    print(f\"\\nEvaluated on {len(true_intents)} samples\")\n",
    "    print(\"System Evaluation:\")\n",
    "    print(f\"Intent Accuracy: {intent_accuracy:.2f}\")\n",
    "    print(f\"Appropriate Response Rate: {response_accuracy:.2f}\")\n",
    "    print(\"\\nIntent Classification Report:\")\n",
    "    print(classification_report(true_intents, pred_intents))\n",
    "else:\n",
    "    print(\"No samples were successfully processed for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cce93-81f6-4b85-8e1c-527c25435c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
